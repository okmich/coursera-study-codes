{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Implement advanced Transformer models using Keras. \n",
    "\n",
    "- Apply Transformers to real-world sequential data tasks. \n",
    "\n",
    "- Build, train, and evaluate Transformer models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.9.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.13.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, pyarrow, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 keras-3.9.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.8 numpy-2.1.3 opt-einsum-3.4.0 optree-0.14.1 protobuf-5.29.3 pyarrow-19.0.1 rich-13.9.4 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.1.0 pyparsing-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 13:33:25.308124: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-17 13:33:25.309684: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-17 13:33:25.314142: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-17 13:33:25.326042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742218405.346143     298 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742218405.352071     298 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742218405.368058     298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742218405.368079     298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742218405.368082     298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742218405.368084     298 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-17 13:33:25.373718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.993428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99.773496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101.395427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103.196135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.731793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close\n",
       "0  100.993428\n",
       "1   99.773496\n",
       "2  101.395427\n",
       "3  103.196135\n",
       "4   99.731793"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 13:35:23.180677: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - loss: 9.0098  \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.1957 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.1482 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.1515 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.1942 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.1280 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.1337 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.1713 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.1262 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.1114 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.1531 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.0849 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.0996 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.0812 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.0722 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 1s/step - loss: 0.0527 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.0577 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.0470 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.0595 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 1s/step - loss: 0.0322 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fc6befe4b90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 339ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    " \n",
    "\n",
    "# Plot the predictions \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(data, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.show() \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "28ac4fd81c1d713f83dcd1cdf1d3383ad25ea92873288fe9e978e9a17b314709"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
